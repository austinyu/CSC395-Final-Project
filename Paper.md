[TOC]



# Introduction



# Materials and Methods

## Proposed Framework.





## Algorithms





### Logistic regression





### Linear SVM



### K-nearest neighbors

K Nearest Neighbours (KNN) is a non-parametric classification algorithm that has been extensively used in many scientific fields due to its simplicity and effectiveness. The algorithm operates by aggregating the outcomes of the k nearest neighbors from the training dataset to classify a new observation. In KNN, the selection of the optimal value of k is a critical step as it directly impacts the trade-off between bias and variance. Specifically, a small value of k leads to high variance and low bias, while a large value of k results in high bias and low variance. To determine the optimal value of k, the cross-validation method is usually employed, which involves splitting the dataset into training and validation sets to assess the performance of the model on new data. Therefore, the selection of the appropriate value of k in KNN is a crucial step in achieving high predictive accuracy and robustness of the model.

### Voting classiﬁer



### Bagging classiﬁer



### CNN



## Benchmark Algorithms.



## Datasets



## Performance Metrics.

# Results and Discussion





# Conclusion 